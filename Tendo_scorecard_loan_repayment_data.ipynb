{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9893b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c60e4",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0f9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc447b5",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e846f",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5422d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceded77",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b52766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f306",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6eb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c57c77",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3600b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acf1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f88c",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b0246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = 'risk_mart'\n",
    "\n",
    "\n",
    "al = f'applied_loans_20230101_{CURRENT_DATE}'\n",
    "altrans = f'applied_loans_20210701_{CURRENT_DATE}_trans'\n",
    "nal = f'tsa_onboarded_but_never_applied_loan_20230101_{CURRENT_DATE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf69e7",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dac210",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.tendo_scorecard_loan_repayment_data_16062025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affef659",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "CREATE OR REPLACE TABLE `prj-prod-dataplatform.worktable_data_analysis.tendo_scorecard_loan_repayment_data_16062025` as\n",
    "WITH paid_payments as (   ----------------more payments belonging one repayment\n",
    "select repayment_schedule_id,\n",
    "sum(amount) as amount,\n",
    "max(repaid_date) as repaid_date\n",
    "from tendopay_raw.split_purchases\n",
    "group by  repayment_schedule_id\n",
    "), \n",
    "payment_channel_data as (\n",
    "SELECT\n",
    "u.id as user_id,\n",
    "pr.id AS loan_id,\n",
    "DENSE_RANK() over (partition by pr.id order by rs.due_date ) as installment_number,\n",
    "vendor_id,\n",
    "subfamily,\n",
    "split_purchases.repaid_date,\n",
    "rs.due_date,\n",
    "rs.principal as due_principal,\n",
    "rs.interest as due_interest,\n",
    "rs.amount as due_amount,\n",
    "split_purchases.principal as paid_principal,\n",
    "split_purchases.interest as paid_interest,\n",
    "split_purchases.amount as paid_amount,\n",
    "outstanding_balance,\n",
    "case\n",
    "     when (pp.repaid_date is not null and rs.amount <= pp.amount) then date_diff(pp.repaid_date, rs.due_date, day)\n",
    "     when (pp.repaid_date is not null and rs.amount > pp.amount) then date_diff(CURRENT_DATE, rs.due_date, day)\n",
    "     when pp.repaid_date is null and due_date < CURRENT_DATE then date_diff(CURRENT_DATE, rs.due_date, day)\n",
    "end as DPD\n",
    "\n",
    "from tendopay_raw.payment_responses pr \n",
    "JOIN tendopay_raw_staging.users u on u.id=pr.tendopay_user_id\n",
    "JOIN tendopay_raw_staging.repayment_schedules rs ON rs.payment_response_id = pr.id\n",
    "JOIN tendopay_raw_staging.split_purchases split_purchases ON cast(rs.id as string) = split_purchases.repayment_schedule_id AND CAST(split_purchases.repaid_date AS date) < CURRENT_DATE()\n",
    "JOIN `tendopay_raw.customer_repayment_responses` crs ON split_purchases.txnid = crs.txn_id\n",
    "join paid_payments pp on cast(rs.id as string)=pp.repayment_schedule_id\n",
    "where u.product_type = 'employer'\n",
    "),\n",
    "ordered_payments AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (\n",
    "            PARTITION BY user_id, loan_id, installment_number \n",
    "            ORDER BY repaid_date, due_date\n",
    "         ) AS payment_seq\n",
    "  FROM payment_channel_data\n",
    "),\n",
    "balance_calc AS (\n",
    "  SELECT *,\n",
    "         -- Get the previous installment's last known outstanding balance\n",
    "         LAG(outstanding_balance) OVER (\n",
    "            PARTITION BY user_id, loan_id \n",
    "            ORDER BY installment_number\n",
    "         ) AS prev_installment_balance\n",
    "  FROM ordered_payments\n",
    "),\n",
    "before_calc AS (\n",
    "  SELECT *,\n",
    "         -- Determine Outstanding Before\n",
    "         CASE \n",
    "           -- First payment of an installment: Use previous installment's balance\n",
    "           WHEN payment_seq = 1 THEN COALESCE(prev_installment_balance, outstanding_balance + paid_amount)\n",
    "           -- Subsequent payments in the same installment: Use previous row's outstanding_after\n",
    "           ELSE LAG(outstanding_balance) OVER (\n",
    "               PARTITION BY user_id, loan_id, installment_number \n",
    "               ORDER BY repaid_date, due_date\n",
    "           )\n",
    "         END AS outstanding_before\n",
    "  FROM balance_calc\n",
    ")\n",
    "SELECT *,\n",
    "FROM payment_channel_data\n",
    "--where loan_id = 3972\n",
    "ORDER BY user_id, loan_id, installment_number, repaid_date;\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f'Table {schema1}.{al} created successfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
