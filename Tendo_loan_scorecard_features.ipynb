{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9893b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "\n",
    "\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c60e4",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0f9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250915\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "print(CURRENT_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58608f1f",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab9642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique Id is: a0439b920677\n"
     ]
    }
   ],
   "source": [
    "unique_id = str(uuid.uuid4()).replace('-', '')[-12:]\n",
    "print(f\"The unique Id is: {unique_id}\")\n",
    "BUCKETNAME = 'prod-asia-southeast1-tonik-aiml-workspace'\n",
    "CLOUDPATH = 'DC/Tendo_Data'\n",
    "LOCALPATH = r'D:\\OneDrive - Tonik Financial Pte Ltd\\MyStuff\\Data Engineering\\Model_Data_Set_preparation\\Tendo_Data_Preparation\\Data'\n",
    "VERSION = 'V1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc447b5",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e846f",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5422d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceded77",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b52766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f306",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6eb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c57c77",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3600b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acf1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840831b2",
   "metadata": {},
   "source": [
    "# save_dataframe_multi_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af8f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_multi_format(\n",
    "    dataframe: pd.DataFrame, \n",
    "    cloud_path: str, \n",
    "    filename: str, \n",
    "    client: bigquery.Client = None,\n",
    "    bucket_name: str = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to Google Cloud Storage in multiple formats (CSV, Pickle, Parquet, Joblib).\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to save\n",
    "        cloud_path (str): The cloud path (e.g., 'DC/Model_Monitoring/cash_beta_trench1_data')\n",
    "        filename (str): The base filename without extension\n",
    "        client (bigquery.Client, optional): BigQuery client (for project reference)\n",
    "        bucket_name (str, optional): GCS bucket name. If None, will try to extract from client\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with status of each file saved\n",
    "        \n",
    "    Example:\n",
    "        client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "        CLOUDPATH = 'DC/Model_Monitoring/cash_beta_trench1_data'\n",
    "        \n",
    "        results = save_dataframe_multi_format(\n",
    "            dataframe=d1,\n",
    "            cloud_path=CLOUDPATH,\n",
    "            filename='my_data',\n",
    "            client=client,\n",
    "            bucket_name='your-bucket-name'  # Replace with your actual bucket name\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Google Cloud Storage client\n",
    "    storage_client = storage.Client(project=client.project if client else None)\n",
    "    \n",
    "    # You'll need to specify your bucket name here\n",
    "    # Common bucket names in GCP data platforms might be like:\n",
    "    # - 'prj-prod-dataplatform-storage'\n",
    "    # - 'dataplatform-storage'\n",
    "    # - or similar pattern\n",
    "    if bucket_name is None:\n",
    "        # You need to replace this with your actual bucket name\n",
    "        raise ValueError(\"Please provide the bucket_name parameter\")\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Results dictionary to track saves\n",
    "    results = {}\n",
    "    \n",
    "    # Ensure cloud_path doesn't start with '/'\n",
    "    cloud_path = cloud_path.lstrip('/')\n",
    "    \n",
    "    try:\n",
    "        # 1. Save as CSV\n",
    "        csv_buffer = io.StringIO()\n",
    "        dataframe.to_csv(csv_buffer, index=False)\n",
    "        csv_blob = bucket.blob(f\"{cloud_path}/{filename}.csv\")\n",
    "        csv_blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')\n",
    "        results['csv'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.csv\"\n",
    "        \n",
    "        # 2. Save as Pickle\n",
    "        pickle_buffer = io.BytesIO()\n",
    "        pickle.dump(dataframe, pickle_buffer)\n",
    "        pickle_blob = bucket.blob(f\"{cloud_path}/{filename}.pkl\")\n",
    "        pickle_blob.upload_from_string(pickle_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['pickle'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.pkl\"\n",
    "        \n",
    "        # 3. Save as Parquet\n",
    "        parquet_buffer = io.BytesIO()\n",
    "        dataframe.to_parquet(parquet_buffer, index=False)\n",
    "        parquet_blob = bucket.blob(f\"{cloud_path}/{filename}.parquet\")\n",
    "        parquet_blob.upload_from_string(parquet_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['parquet'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.parquet\"\n",
    "        \n",
    "        # 4. Save as Joblib\n",
    "        joblib_buffer = io.BytesIO()\n",
    "        joblib.dump(dataframe, joblib_buffer)\n",
    "        joblib_blob = bucket.blob(f\"{cloud_path}/{filename}.joblib\")\n",
    "        joblib_blob.upload_from_string(joblib_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['joblib'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.joblib\"\n",
    "        \n",
    "        print(\"All files saved successfully!\")\n",
    "        for format_type, path in results.items():\n",
    "            print(f\"{format_type.upper()}: {path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f88c",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b0246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tendo_scorecard_features_data_20250915\n"
     ]
    }
   ],
   "source": [
    "schema1 = 'worktable_data_analysis'\n",
    "\n",
    "tendoscorecardfeature = f'tendo_scorecard_features_data_{CURRENT_DATE}'\n",
    "print(tendoscorecardfeature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d4199",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161cab7",
   "metadata": {},
   "source": [
    "# worktable_data_analysis.tendo_scorecard_features_data_16062025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f7ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table worktable_data_analysis.tendo_scorecard_features_data_20250915 created\n"
     ]
    }
   ],
   "source": [
    "sq = f\"\"\"\n",
    "create or replace table {schema1}.{tendoscorecardfeature} as \n",
    "with paid_payments as (   ----------------more payments belonging one repayment\n",
    "select repayment_schedule_id,\n",
    "sum(amount) as amount,\n",
    "max(repaid_date) as repaid_date\n",
    "from tendopay_raw.split_purchases\n",
    "group by  repayment_schedule_id\n",
    "),\n",
    "delinquency as (\n",
    "select u.id as user_id, u.product_type,\n",
    "pr.id as loan_id,\n",
    "pr.principal, pr.fee, pr.bir, pr.tendopay_installments as term,\n",
    "substr(cast(date_trunc(pr.created_at, MONTH) as string),0,7) as mth,\n",
    "rs.due_date,\n",
    "RANK() over (partition by pr.id order by rs.due_date ) as installmentNumber,\n",
    "rs.amount as annuity,\n",
    "pp.amount as pp_amout, pp.repaid_date,\n",
    "rs.outstanding_balance,\n",
    "case when rs.amount - pp.amount > 0 then 1 else 0 end as flag_part_payment,\n",
    "case\n",
    "     when (repaid_date is not null and rs.amount <= pp.amount) then date_diff(pp.repaid_date, rs.due_date, day)\n",
    "     when (repaid_date is not null and rs.amount > pp.amount) then date_diff(CURRENT_DATE, rs.due_date, day)\n",
    "     when repaid_date is null and due_date < CURRENT_DATE then date_diff(CURRENT_DATE, rs.due_date, day)\n",
    "end as DPD\n",
    "from tendopay_raw.payment_responses pr\n",
    "join tendopay_raw.users u on pr.tendopay_user_id=u.id\n",
    "join tendopay_raw.repayment_schedules rs on rs.payment_response_id=pr.id\n",
    "---all repayment schudeles joined\n",
    "left join paid_payments pp on cast(rs.id as string)=pp.repayment_schedule_id\n",
    "--all paid payment joined\n",
    "where tendopay_disposition = 'success' and status='PTOK' --Status 'PTOK' is a successful loan, others are cancelled loans.\n",
    ")\n",
    "--------calculation of delinquencies\n",
    ",min_inst_obs_def as (\n",
    "select delinquency.user_id, delinquency.product_type, delinquency.loan_id,\n",
    "min(DATE(due_date)) as min_loan_due_date,\n",
    "max(installmentNumber) as max_installmentNumber, avg(principal) as principal, avg(annuity) as annuity,\n",
    "  MIN(CASE WHEN installmentNumber = 1 THEN annuity END) AS first_outstanding_due_amount,\n",
    "  MIN(CASE WHEN installmentNumber = 1 THEN outstanding_balance END) AS first_outstanding_balance,\n",
    "  MIN(CASE WHEN installmentNumber = 2 THEN outstanding_balance END) AS second_outstanding_balance,\n",
    "  MIN(CASE WHEN installmentNumber = 3 THEN outstanding_balance END) AS third_outstanding_balance,\n",
    "  MIN(CASE WHEN installmentNumber = 4 THEN outstanding_balance END) AS fourth_outstanding_balance,\n",
    "  MIN(CASE WHEN DPD >0 THEN installmentNumber  END  ) AS min_inst_def0,\n",
    "  MIN(CASE WHEN DPD >=10 THEN installmentNumber  END ) AS min_inst_def10,\n",
    "  MIN(CASE WHEN DPD >=30 THEN installmentNumber  END ) AS min_inst_def30,\n",
    "  MIN(CASE WHEN DPD >=60 THEN installmentNumber   END ) AS min_inst_def60,\n",
    "  MIN(CASE WHEN DPD >=90 THEN installmentNumber  END ) AS min_inst_def90,\n",
    "  MIN(CASE WHEN DPD >=180 THEN installmentNumber END ) AS min_inst_def180,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) > 0 then installmentNumber end) as obs_min_inst_def0,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) >= 10 then installmentNumber end) as obs_min_inst_def10,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) >= 30 then installmentNumber end) as obs_min_inst_def30,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) >= 60 then installmentNumber end) as obs_min_inst_def60,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) >= 90 then installmentNumber end) as obs_min_inst_def90,\n",
    "  max(case when DATE_DIFF(CURRENT_DATE,due_date, DAY) >= 180 then installmentNumber end) as obs_min_inst_def180\n",
    "from delinquency\n",
    "group by user_id, product_type, loan_id\n",
    "),\n",
    "fspd_data as (\n",
    "SELECT\n",
    "pr.id as loan_id,\n",
    "min_loan_due_date,\n",
    "sum(case when obs_min_inst_def30>=1 then 1 else 0 end) as obs_FPD30,\n",
    "sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 then 1 else 0 end) as def_FPD30,\n",
    "sum(case when obs_min_inst_def30>=2 then 1 else 0 end) as obs_SPD30,\n",
    "sum(case when obs_min_inst_def30>=2 and min_inst_def30=2 then 1 else 0 end) as def_SPD30,\n",
    "sum(case when obs_min_inst_def30>=3 then 1 else 0 end) as obs_TPD30,\n",
    "sum(case when obs_min_inst_def30>=3 and min_inst_def30=3 then 1 else 0 end) as def_TPD30,\n",
    "sum(case when obs_min_inst_def30>=1 then pr.principal else 0 end) as obs_FPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=4 then 1 else 0 end) as obs_FSTFPD30,\n",
    "sum(case when obs_min_inst_def30>=4 and min_inst_def30=4 then 1 else 0 end) as def_FSTFPD30,\n",
    "sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 and cast(pr.tendopay_installments as string) = '1' then first_outstanding_due_amount \n",
    "when obs_min_inst_def30>=1 and min_inst_def30=1 then first_outstanding_balance\n",
    "else 0 end) as def_FPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=2 then pr.principal else 0 end) as obs_SPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=2 and min_inst_def30=2 then second_outstanding_balance else 0 end) as def_SPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=3 then pr.principal else 0 end) as obs_TPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=3 and min_inst_def30=3 then third_outstanding_balance else 0 end) as def_TPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=4 then pr.principal else 0 end) as obs_FSTFPD30_vol,\n",
    "sum(case when obs_min_inst_def30>=4 and min_inst_def30=4 then fourth_outstanding_balance else 0 end) as def_FSTFPD30_vol,\n",
    "FROM tendopay_raw.payment_responses pr\n",
    "join tendopay_raw.users u on pr.tendopay_user_id=u.id\n",
    "join min_inst_obs_def def on def.loan_id=pr.id\n",
    "group by 1,2\n",
    "),\n",
    "cl_fspd_data as (\n",
    "SELECT\n",
    "u.id as user_id,\n",
    "IF(DATE_DIFF(CURRENT_DATE(),MIN(DATE(pr.created_at)),DAY)>=60,1,0) AS cl_matured_fpd30_flag,\n",
    "IF(DATE_DIFF(CURRENT_DATE(),MIN(DATE(pr.created_at)),DAY)>=90,1,0) AS cl_matured_fspd30_flag,\n",
    "IF(DATE_DIFF(CURRENT_DATE(),MIN(DATE(pr.created_at)),DAY)>=120,1,0) AS cl_matured_fstpd30_flag,\n",
    "IF(DATE_DIFF(CURRENT_DATE(),MIN(DATE(pr.created_at)),DAY)>=150,1,0) AS cl_matured_fstfpd30_flag,\n",
    "CASE WHEN sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 then 1 else 0 end) >= 1 THEN 1 ELSE 0 END AS cl_fpd30_flag,\n",
    "\n",
    "CASE WHEN sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 then 1 else 0 end) >= 1 OR sum(case when obs_min_inst_def30>=2 and min_inst_def30=2 then 1 else 0 end) >=1  THEN 1\n",
    "ELSE 0\n",
    "END cl_fspd30_flag,\n",
    "CASE WHEN sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 then 1 else 0 end) >= 1 OR sum(case when obs_min_inst_def30>=2 and min_inst_def30=2 then 1 else 0 end) >=1 OR sum(case when obs_min_inst_def30>=3 and min_inst_def30=3 then 1 else 0 end) >= 1 THEN 1\n",
    "ELSE 0\n",
    "END cl_fstpd30_flag,\n",
    "CASE WHEN sum(case when obs_min_inst_def30>=1 and min_inst_def30=1 then 1 else 0 end) >= 1 OR sum(case when obs_min_inst_def30>=2 and min_inst_def30=2 then 1 else 0 end) >=1 OR sum(case when obs_min_inst_def30>=3 and min_inst_def30=3 then 1 else 0 end) >= 1 OR sum(case when obs_min_inst_def30>=4 and min_inst_def30=4 then 1 else 0 end) >= 1 THEN 1\n",
    "ELSE 0\n",
    "END cl_fstfpd30_flag,\n",
    "FROM tendopay_raw.payment_responses pr\n",
    "join tendopay_raw.users u on pr.tendopay_user_id=u.id\n",
    "join min_inst_obs_def def on def.loan_id=pr.id\n",
    "group by 1\n",
    "),\n",
    "payment_channel_data as (\n",
    "SELECT\n",
    "pr.id AS loan_id,\n",
    "STRING_AGG(DISTINCT vendor_id,'|')\n",
    "from\n",
    "tendopay_raw.customer_repayment_responses crs\n",
    "JOIN tendopay_raw.split_purchases ON split_purchases.txnid = crs.txn_id\n",
    "join tendopay_raw.repayment_schedules rs on cast(rs.id as string)=split_purchases.repayment_schedule_id\n",
    "JOIN tendopay_raw.payment_responses pr on rs.payment_response_id=pr.id\n",
    "GROUP BY 1\n",
    "),\n",
    "frozen_tags as (\n",
    "SELECT user_id,\n",
    "max(case when tag_id in (39, 100, 101, 102, 103) then 1 ELSE 0 END ) as frozen_tag\n",
    "FROM `tendopay_raw.user_tag` \n",
    "group by user_id\n",
    "),\n",
    "data_preparation as (\n",
    "SELECT u.id                                                                                    user_id,\n",
    "       u.created_at                                                                            sign_up_date,\n",
    "       ut.first_account_activated_at                                                           approval_date,\n",
    "       u.employer_id                                                                           employer_id,\n",
    "       e.name                                                                                  employer_name,\n",
    "       ii.employment_date                                                                      employment_date,\n",
    "       LENGTH(employment_date)                                                                 LENGTH_employment_date,\n",
    "       datetime_diff(cast(ii.created_at as date), \n",
    "\t   case               ------original: datetime_diff(cast(ut.first_account_activated_at as date),\n",
    "        when LENGTH(employment_date)=6 then   date( cast(substr(employment_date, 1, 4) as int64), cast(substr(employment_date, 6, 1) as int64), 1)\n",
    "        when LENGTH(employment_date)=7 then   date( cast(substr(employment_date, 1, 4) as int64), cast(substr(employment_date, 6, 2) as int64), 1)\n",
    "        when LENGTH(employment_date)=10 then  date( cast(substr(employment_date, 1, 4) as int64), cast(substr(employment_date, 6, 2) as int64), cast(substr(employment_date, 9, 2) as int64))\n",
    "      end, day) / 365 as employer_time,    ----sometimes employment_date is missing, sometimes approval_date (first_account_activated_at) is missing\n",
    "       u.gender                                                                                gender,\n",
    "       u.civil_status                                                                          civil_status,\n",
    "       ii.employee_status                                                                      employment_status,\n",
    "      case\n",
    "        when ii.income in ('0-10000') then 5000\n",
    "        when ii.income in ('10000-20000') then 15000\n",
    "        when ii.income in ('20000-30000') then 25000\n",
    "        when ii.income in ('30000-40000') then 35000\n",
    "        when ii.income in ('40000-50000') then 45000\n",
    "        when ii.income in ('50000+') then 50000\n",
    "        else cast(ii.income as numeric)\n",
    "      end as                                                                                   declared_income_num, \n",
    "       ii.verified_net_income                                                                  verified_net_income,\n",
    "       case when tag.frozen_tag = 1 then 'Frozen' ELSE 'Not Frozen' END    Frozen_Status\n",
    "       /*\n",
    "       CASE\n",
    "           WHEN   (SELECT COUNT(tag_id) FROM `tendopay_raw.user_tag` WHERE tag_id = 100 AND user_id = u.id) > 0\n",
    "               OR (SELECT COUNT(tag_id) FROM `tendopay_raw.user_tag` WHERE tag_id = 101 AND user_id = u.id) > 0\n",
    "               OR (SELECT COUNT(tag_id) FROM `tendopay_raw.user_tag` WHERE tag_id = 102 AND user_id = u.id) > 0\n",
    "               OR (SELECT COUNT(tag_id) FROM `tendopay_raw.user_tag` WHERE tag_id = 103 AND user_id = u.id) > 0\n",
    "               OR (SELECT COUNT(tag_id) FROM `tendopay_raw.user_tag` WHERE tag_id = 39 AND user_id = u.id) > 0\n",
    "               THEN 'Frozen'\n",
    "           ELSE 'Not Frozen' END                                                               Frozen_Status,\n",
    "       --cci.value                                                                               Credit_limit,\n",
    "\n",
    "        */\n",
    "FROM `tendopay_raw.users` u\n",
    "         LEFT JOIN `tendopay_raw.income_info` ii ON u.id = ii.user_id\n",
    "         LEFT JOIN `tendopay_raw.user_timelines` ut ON u.id = ut.user_id\n",
    "         LEFT JOIN `tendopay_raw.employers` e on u.employer_id = cast(e.id as string)\n",
    "         LEFT JOIN frozen_tags tag on u.id = tag.user_id\n",
    "         --LEFT JOIN customer_credit_information cci on u.id = cci.user_id\n",
    "WHERE u.product_type in ('employer', 'payroll')\n",
    "  --and u.account_activated = 2\n",
    "  --and cci.`key` = 'credit-limit';\n",
    "),\n",
    "scoring_preparation as (\n",
    "select  \n",
    "  dp.*, \t\t\t\t\t\t\t\t\t\n",
    "  case \n",
    "    when\temployer_time\t\t<\t0.55\tthen\t98\t\t\t\n",
    "    when\temployer_time\t\t<\t2.35\tthen\t123\t\t\t\n",
    "    when\temployer_time\t\t<\t3.85\tthen\t139\t\t\t\n",
    "    when\temployer_time\t\t>=\t3.85\tthen\t183\t\t\t\n",
    "    when\temployer_time\t\tis null\t\tthen\t119\t\t\t\n",
    "    else\t\t\t\t\t119\t\n",
    "  end as \temployer_time_score,\t\n",
    "  case \n",
    "    when\tgender\t\tin (\t'Female'\t ) then\t120\t\t\t\n",
    "    when\tgender\t\tin (\t'not specified', 'Male'\t ) then\t118\t\t\t\n",
    "    when\tgender\t\tis null\t\tthen\t119\t\t\t\n",
    "    else\t\t\t\t\t119\t\n",
    "  end as \tgender_score,\t\n",
    "  case \n",
    "    when\tdeclared_income_num\t\t<\t21200\tthen\t119\t\t\t\n",
    "    when\tdeclared_income_num\t\t<\t26525\tthen\t110\t\t\t\n",
    "    when\tdeclared_income_num\t\t<\t33050\tthen\t118\t\t\t\n",
    "    when\tdeclared_income_num\t\t>=\t33050\tthen\t146\t\t\t\n",
    "    when\tdeclared_income_num\t\t<\t0\tthen\t119\t\t\t\n",
    "    when\tdeclared_income_num\tis null\t\tthen\t119\t\t\t\n",
    "    else\t\t\t119\t\n",
    "  end as \tdeclared_income_score,\t\n",
    "  case \n",
    "    when\tcivil_status\t\tis null\t\tthen\t127\t\t\t\n",
    "    when\tcivil_status\t\tin (\t'Divorced','Married','Widowed'\t ) then\t127\t\t\t\n",
    "    when\tcivil_status\t\tin (\t'Single'\t ) then\t117\t\t\t\n",
    "    else\t\t119\t\n",
    "  end as \tcivil_status_score\n",
    "from data_preparation dp\n",
    "),scoring as (\n",
    "select  \n",
    "  sp.*, \n",
    "  employer_time_score+gender_score+declared_income_score+civil_status_score as score\n",
    "from scoring_preparation sp\n",
    "),\n",
    "rating as (\n",
    "select \n",
    "  user_id,\n",
    "  frozen_status,\n",
    "  employer_time_score,gender_score,declared_income_score,civil_status_score,\n",
    "  case \n",
    "    when score > 532 then 'A'\n",
    "    when score > 492 then 'B'\n",
    "    when score > 478 then 'C'\n",
    "    when score > 468 then 'D'\n",
    "    when score > 452 then 'E'\n",
    "    when score<= 452 then 'F'\n",
    "    else null\n",
    "  end as credit_rating  \n",
    "from scoring sco\n",
    ")\n",
    "SELECT\n",
    "--Employees related data\n",
    "users.id as ee_customer_id,\n",
    "users.email as ee_email,\n",
    "users.phone as ee_phone_number,\n",
    "users.firstname as ee_firstname,\n",
    "users.middlename as ee_middlename,\n",
    "users.lastname as ee_lastname,\n",
    "users.birthdate ee_birthdate,\n",
    "users.gender ee_gender,\n",
    "users.email_verified as ee_email_verified_flag,\n",
    "users.telephone_verified as ee_telephone_verified_flag,\n",
    "users.id_verified as ee_id_verified_flag,\n",
    "users.income_verified as ee_income_verified_flag,\n",
    "users.morning_time as ee_morning_time_contact_time,\n",
    "users.afternoon_time as ee_afternoon_time_contact_time,\n",
    "users.account_activated as ee_account_activated_flag,\n",
    "address.region_name ee_region_name,\n",
    "city_name ee_city_name,\n",
    "COALESCE(barangay_name) ee_barangay,\n",
    "address.address_line_1 as ee_address_line_1,\n",
    "address.address_line_2 as ee_address_line_2,\n",
    "COALESCE(id_info.postal_code) ee_postal_code,\n",
    "COALESCE(id_info.landmark,address.landmark) ee_landmark,\n",
    "id_info.residing_date ee_residing_date,\n",
    "income_info.employee_status as ee_employment_status,\n",
    "users.civil_status ee_civil_status,\n",
    "doc_type.name as ee_kyc_doc_name,\n",
    "id_info.is_citizen ee_is_citizen_flag,\n",
    "user_timelines.first_account_activated_at as ee_onboarding_date,\n",
    "COALESCE(hired_date) as ee_employment_date,\n",
    "customer_status as ee_fraud_status,\n",
    "income_info.contract ee_job_type,\n",
    "employer_employees.comment as ee_comment,\n",
    "employer_employees.department as ee_department,\n",
    "employer_employees.recommended_ir as ee_recommended_ir,\n",
    "income_info.job_title as ee_job_title,\n",
    "employment_ids.name as ee_employment_type,\n",
    "income_info.nature_of_work as ee_nature_of_work,\n",
    "freeze_tag.created_at as ee_permanent_freeze_date,\n",
    "DATE(user_deleted_at) ee_resignation_date,\n",
    "users.product_type as ee_product_type,\n",
    "frozen_status as ee_frozen_status,\n",
    "employer_time_score as cust_risk_employer_time_score_v1,\n",
    "gender_score cust_risk_gender_score_v1,\n",
    "declared_income_score cust_risk_declared_income_score_v1,\n",
    "civil_status_score cust_risk_civil_status_score_v1,\n",
    "(employer_time_score+gender_score+declared_income_score+civil_status_score) as cust_risk_combined_score_v1,\n",
    "credit_rating cust_risk_cat_v1,\n",
    "--employers related data\n",
    "employers.id as er_employer_id,\n",
    "employer_group.group_id as er_employer_group_id,\n",
    "employers.name as er_employer_name,\n",
    "employers.email_domain as er_email_domain,\n",
    "employers.repayment_days er_repayment_days_month,\n",
    "employers.custom_email as er_custom_email,\n",
    "employers.payment_reminders as er_payment_reminders,\n",
    "employers.address as er_address,\n",
    "employers.postal_code_id as er_postal_code_id,\n",
    "employers.max_bir as er_max_base_interest_rate,\n",
    "employers.industry er_employer_industry,\n",
    "CASE WHEN employers.status  = 1 THEN 'ACTIVATED'\n",
    "WHEN employers.status  = 2 THEN 'IN_PROGRESS' \n",
    "WHEN employers.status  = 3 THEN 'SUSPENDED' \n",
    "WHEN employers.status  = 4 THEN 'PARKED' \n",
    "END AS er_employer_status,\n",
    "employers.activated_at as er_activated_at,\n",
    "employers.deleted_at as er_deleted_at,\n",
    "employers.created_at as er_created_at,\n",
    "employers.updated_at as er_updated_at,\n",
    "\n",
    "--credit line data\n",
    "kyc_credit_info.monthly_utility_bills_amount cl_monthly_utility_bills_amount,\n",
    "income_info.verified_gross_income as cl_monthly_income_gross,\n",
    "income_info.verified_net_income as cl_monthly_income_net,\n",
    "CASE WHEN REGEXP_CONTAINS(model_has_permissions.model_type, r'App\\\\\\\\User') AND model_has_permissions.permission_id =2 THEN 1\n",
    "ELSE 0\n",
    "END AS cl_multiple_purchases_enabled_flag,\n",
    "employers.max_credit_limit as cl_max_credit_limit_multiplier,\n",
    "max_debt_income_ratio cl_max_debt_income_ratio,\n",
    "cl_matured_fpd30_flag,\n",
    "cl_matured_fspd30_flag,\n",
    "cl_matured_fstpd30_flag,\n",
    "cl_matured_fstfpd30_flag,\n",
    "cl_fpd30_flag,\n",
    "cl_fspd30_flag,\n",
    "cl_fstpd30_flag,\n",
    "cl_fstfpd30_flag,\n",
    "\n",
    "--loan and repayment data\n",
    "pr.id as ln_loan_id,\n",
    "CASE WHEN pr.merchant_id = 423 THEN 'Tendo Plus'\n",
    "when pr.id is not null then 'Tendo'\n",
    "END AS ln_loan_type,\n",
    "CASE \n",
    "WHEN pr.status = 'AUOK' THEN 'Authorized'\n",
    "WHEN pr.status = 'PTOK' THEN 'Approved/Disbursed'\n",
    "WHEN pr.status = 'CTOK' THEN 'Approved Transaction Cancelled'\n",
    "WHEN pr.status = 'AUCA' THEN 'Authorization Cancelled'\n",
    "WHEN pr.status = 'PTNG' THEN 'Rejected'\n",
    "WHEN pr.status = 'PTCA' THEN 'Cancelled'\n",
    "END AS ln_loan_status,\n",
    "xendit_payment_responses.channel_code as ln_disbursement_channel,\n",
    "pr.principal ln_original_principal, \n",
    "pr.fee ln_orig_interest_fees, \n",
    "pr.tendopay_installments as ln_orig_tenor,\n",
    "pr.created_at ln_loan_application_datetime,\n",
    "pr.repaid_full as ln_repaid_full_flag,\n",
    "DATE(pr.fully_repaid_at) ln_fully_repaid_date,\n",
    "CASE WHEN def_FPD30 = 1 THEN 1\n",
    "ELSE 0\n",
    "END ln_fpd30_flag,\n",
    "CASE WHEN def_FPD30 = 1 OR def_SPD30 =1 THEN 1\n",
    "ELSE 0\n",
    "END ln_fspd30_flag,\n",
    "CASE WHEN def_FPD30 = 1 OR def_SPD30 =1 OR def_TPD30 = 1 THEN 1\n",
    "ELSE 0\n",
    "END ln_fstpd30_flag,\n",
    "CASE WHEN def_FPD30 = 1 OR def_SPD30 =1 OR def_TPD30 = 1 OR def_FSTFPD30 = 1 THEN 1\n",
    "ELSE 0\n",
    "END ln_fstfpd30_flag,\n",
    "min_loan_due_date as ln_min_loan_due_date,\n",
    "def_FPD30_vol AS ln_os_principal_at_fpd30,\n",
    "def_SPD30_vol AS ln_os_principal_at_fspd30,\n",
    "def_TPD30_vol AS ln_os_principal_at_fstpd30,\n",
    "def_FSTFPD30_vol AS ln_os_principal_at_fstfpd30,\n",
    "\n",
    "CASE WHEN DATE_DIFF(CURRENT_DATE(),min_loan_due_date,DAY) >= 30 THEN 1 ELSE 0 END AS ln_matured_fpd30_flag, \n",
    "CASE WHEN DATE_DIFF(CURRENT_DATE(),min_loan_due_date,DAY) >= 60 THEN 1 ELSE 0 END AS ln_matured_fspd30_flag, \n",
    "CASE WHEN DATE_DIFF(CURRENT_DATE(),min_loan_due_date,DAY) >= 90 THEN 1 ELSE 0 END AS ln_matured_fstpd30_flag, \n",
    "CASE WHEN DATE_DIFF(CURRENT_DATE(),min_loan_due_date,DAY) >= 120 THEN 1 ELSE 0 END AS ln_matured_fstfpd30_flag, \n",
    "\n",
    "from\n",
    "tendopay_raw.users \n",
    "LEFT JOIN tendopay_raw.income_info on income_info.user_id = users.id\n",
    "LEFT JOIN (SELECT * FROM tendopay_raw.employer_employees\n",
    "QUALIFY ROW_NUMBER() OVER(PARTITION BY user_id order by updated_at desc) = 1) employer_employees\n",
    "ON employer_employees.user_id = users.id --and employer_employees.user_deleted_at is null\n",
    "LEFT JOIN (select document_ids.name, files.user_id\n",
    "from tendopay_raw.document_ids\n",
    "join tendopay_raw.files on files.doc_id = document_ids.type\n",
    "where doc_type = 1 AND  REGEXP_CONTAINS(files.owner_type, r'App\\\\\\\\IdInfo') QUALIFY ROW_NUMBER() OVER (PARTITION BY user_id order by updated_at DESC)=1 ) doc_type on doc_type.user_id = users.id\n",
    "LEFT JOIN  tendopay_raw.id_info  on id_info.user_id = users.id\n",
    "LEFT JOIN (select region.name as region_name,barangay.name as barangay_name,\n",
    "address.user_id,address_line_1,address_line_2,landmark\n",
    "FROM `tendopay_raw.address` address\n",
    "LEFT JOIN tendopay_raw.barangay barangay ON barangay.id = address.barangay_id\n",
    "LEFT JOIN tendopay_raw.cities_v2 AS city ON city.id = barangay.city_id\n",
    "LEFT JOIN tendopay_raw.provinces_v2 AS province ON province.id = city.province_id\n",
    "LEFT JOIN tendopay_raw.regions_v2 AS region ON region.id = province.region_id\n",
    "where address.deleted_at IS NULL\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY address.user_id ORDER BY created_at desc) = 1) address on address.user_id = users.id\n",
    "LEFT JOIN  tendopay_raw.user_timelines  on user_timelines.user_id = users.id\n",
    "LEFT JOIN  tendopay_raw.kyc_credit_info  on kyc_credit_info.user_id = users.id\n",
    "LEFT JOIN tendopay_raw.employers on CAST(employers.id as string) = users.employer_id\n",
    "LEFT JOIN tendopay_raw.employer_group ON employer_group.employer_id = employers.id\n",
    "LEFT JOIN tendopay_raw.employment_ids ON employment_ids.id = income_info.employment_id\n",
    "--LEFT JOIN tendopay_raw.tp_groups ON tp_groups.id = employer_group.group_id\n",
    "LEFT JOIN tendopay_raw.payment_responses pr on pr.tendopay_user_id=users.id\n",
    "LEFT JOIN tendopay_raw.model_has_permissions on model_has_permissions.model_id  = users.id\n",
    "LEFT JOIN (SELECT model_id,JSON_EXTRACT_SCALAR(PARSE_JSON(OPTIONS),'$.max_debt_income_ratio') max_debt_income_ratio FROM tendopay_raw.model_has_tp_events WHERE tp_event = \"E0000011\") model_has_tp_events  ON model_has_tp_events.model_id = employers.id\n",
    "LEFT JOIN tendopay_raw.xendit_payment_responses ON xendit_payment_responses.reference_id  = pr.merchant_order_id\n",
    "LEFT JOIN (\n",
    "SELECT \n",
    "user_id,\n",
    "CASE WHEN SUM(CASE WHEN tag_id IN (2,111,113,112,39,106,107,102,100,101,103) AND deleted_at is null THEN 1 ELSE NULL END) > 0  THEN 'Risk'\n",
    "ELSE 'Normal'\n",
    "end as customer_status\n",
    "from\n",
    "tendopay_raw.user_tag\n",
    "GROUP BY user_id\n",
    ") user_tag on user_tag.user_id = users.id\n",
    "LEFT JOIN (\n",
    "SELECT user_id,created_at\n",
    "from\n",
    "tendopay_raw.user_tag\n",
    "where tag_name = 'FREEZE_PERMANENT' and deleted_at is null\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY user_id order by updated_at desc) = 1\n",
    ") freeze_tag on freeze_tag.user_id = users.id\n",
    "LEFT JOIN payment_channel_data ON payment_channel_data.loan_id = pr.id\n",
    "--LEFT JOIN tendopay_raw.payment_requests on payment_requests.user_id = users.id\n",
    "--LEFT JOIN tendopay_raw.payment_response_failures on payment_response_failures.payment_request_id = payment_requests.id\n",
    "LEFT JOIN fspd_data ON fspd_data.loan_id = pr.id\n",
    "LEFT JOIN rating on rating.user_id = users.id\n",
    "LEFT JOIN cl_fspd_data ON cl_fspd_data.user_id = users.id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "job = client.query(sq)\n",
    "job.result()  # Wait for the job to complete.\n",
    "time.sleep(5) # Delays for 30 seconds\n",
    "print(f\"Table {schema1}.{tendoscorecardfeature} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f9c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
